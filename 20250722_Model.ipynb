{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5f7aa3-4f71-46d1-8ec7-c9d3fab157e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "    if 'dataset' in globals():\n",
    "        del globals()['dataset']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece08107-44e6-4046-8eee-1d5877a1ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfa60d6-ef59-4734-9b5e-0e0b602df042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef5732a-e705-4922-8c8d-916de30052d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map={\"\":0})\n",
    "\n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16,  device_map={\"\":0})\n",
    "        lora_config = LoraConfig(\n",
    "                    r=8,\n",
    "                    lora_alpha=32,\n",
    "                    target_modules=[\n",
    "                                        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "                                    ],\n",
    "                    lora_dropout=0.05,\n",
    "                    bias=\"none\",\n",
    "                    task_type=\"CAUSAL_LM\"\n",
    "                )\n",
    "\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582faefc-81fc-4b45-afc7-4a518f116375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPU 메모리 사용량: {used_memory:.3f} GB\")\n",
    "    else:\n",
    "        print(\"런타임 유형을 GPU로 변경하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a35be251-6f33-4c69-8f48-671d9812b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Bllossom/llama-3.2-Korean-Bllossom-AICA-5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c37c1d4-63d7-4613-9ce2-af781452684b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10734aeb52c74a1b85f1a2c243602a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e259217259f406f96ebb60a0e69ee9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,196,160 || all params: 4,326,660,878 || trainable%: 0.3512\n",
      "GPU 메모리 사용량: 8.088 GB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(model_id, peft='lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312bba6a-355b-48bd-a775-05acbff76abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 102612, 114784, 121520, 123151, 105807], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"심장이 매우 뛰어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc41ada2-eb47-4734-bbef-306208a2486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15196160\n",
      "all params: 4326660878\n",
      "trainable%: 0.3512214252165848\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(f\"\"\"trainable params: {trainable_params}\n",
    "all params: {all_param}\n",
    "trainable%: {100 * trainable_params / all_param}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3289d0af-baf9-4198-b944-023b3523822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf1fd00-045b-451c-963f-12fc4ce89d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"csv\", data_files={\"train\" : \"./train_data.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38e9a1e-63b1-4c3e-8dae-716bedf29550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.map(lambda x : {'text' : f\"\"\"User: {x['question']} \n",
    "                    Assistant: {x['answer']}<|endoftext|>\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df8faa74-0d8c-4257-9074-707cf0c83649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 10세 소년이 충치 치료를 위해 치과에 방문했다. 치과의사는 수복 재료로 아말감 대신 복합 레진을 권장하고 있다. 복합 레진 사용의 장점으로 가장 적절한 것은?  \n",
      "1) 심미성이 좋다.  \n",
      "2) 수복물의 강도가 높다.  \n",
      "3) 수은 함유로 인한 독성 위험이 있다.  \n",
      "4) 비용이 저렴하다.  \n",
      "5) 이차 우식증 발생 위험이 낮다. \n",
      "                    Assistant: 1) 심미성이 좋다.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for x in data2['train']:\n",
    "    print(x['text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9ded74-91cd-4153-9cbb-90503f8c7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9accc1b-f019-479b-8037-8c9686bb3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "argu = TrainingArguments(\n",
    "         per_device_train_batch_size=1,\n",
    "        num_train_epochs=2, \n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate = 0.0001,\n",
    "        fp16=True,\n",
    "        output_dir=\"./output/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62725abe-8e76-4845-b63b-7bd0f9721b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data2.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "703aefc7-44b4-48dc-bacb-3809ced737a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb17cae-68f3-4f99-b9d6-c74bbe962dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    args=argu,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137510d-542a-4a1c-8828-7e5c601b4379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1513' max='6114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1513/6114 20:56 < 1:03:45, 1.20 it/s, Epoch 0.49/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c10aab-cb33-40be-a8c0-54eca3e869e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
