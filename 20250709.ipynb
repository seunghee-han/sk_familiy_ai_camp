{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12414556,"sourceType":"datasetVersion","datasetId":7829595}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd \nimdb_df = pd.read_pickle(\"/kaggle/input/imdb-dataset/imdb_preprocessing.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:11:38.087602Z","iopub.execute_input":"2025-07-09T02:11:38.088303Z","iopub.status.idle":"2025-07-09T02:11:38.509547Z","shell.execute_reply.started":"2025-07-09T02:11:38.088277Z","shell.execute_reply":"2025-07-09T02:11:38.508354Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:11:41.141727Z","iopub.execute_input":"2025-07-09T02:11:41.142149Z","iopub.status.idle":"2025-07-09T02:11:41.147231Z","shell.execute_reply.started":"2025-07-09T02:11:41.142123Z","shell.execute_reply":"2025-07-09T02:11:41.146148Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"embedding = nn.Embedding(num_embeddings=146, \n            embedding_dim=10,\n            padding_idx=0)\ntext_encoded = torch.LongTensor(imdb_df.iloc[0, -1])\nresult = embedding(text_encoded)\n\nimdb_df['label'] = imdb_df.sentiment.apply(lambda x : 1 if x == 'positive' else 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:11:41.493395Z","iopub.execute_input":"2025-07-09T02:11:41.493700Z","iopub.status.idle":"2025-07-09T02:11:41.522859Z","shell.execute_reply.started":"2025-07-09T02:11:41.493679Z","shell.execute_reply":"2025-07-09T02:11:41.521856Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass IMDBDataset(Dataset):\n    def __init__(self, df):\n        self.X = df.token.values\n        self.y = df.label.values\n        \n    def __getitem__(self, idx):\n        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n        \n    def __len__(self):\n        return len(self.y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:11:41.756386Z","iopub.execute_input":"2025-07-09T02:11:41.756687Z","iopub.status.idle":"2025-07-09T02:11:41.763684Z","shell.execute_reply.started":"2025-07-09T02:11:41.756665Z","shell.execute_reply":"2025-07-09T02:11:41.762299Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df  = train_test_split(imdb_df, test_size=0.2, random_state=42)\n\ntrain_dataset = IMDBDataset(train_df)\ntest_dataset = IMDBDataset(test_df)\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:11:41.987400Z","iopub.execute_input":"2025-07-09T02:11:41.987699Z","iopub.status.idle":"2025-07-09T02:11:42.036596Z","shell.execute_reply.started":"2025-07-09T02:11:41.987679Z","shell.execute_reply":"2025-07-09T02:11:42.035518Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd \nimdb_df = pd.read_pickle(\"/kaggle/input/imdb-dataset/imdb_preprocessing.pkl\")\n\nimport torch\nimport torch.nn as nn\n\n\nembedding = nn.Embedding(num_embeddings=146, \n            embedding_dim=10,\n            padding_idx=0)\ntext_encoded = torch.LongTensor(imdb_df.iloc[0, -1])\nresult = embedding(text_encoded)\n\nimdb_df['label'] = imdb_df.sentiment.apply(lambda x : 1 if x == 'positive' else 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:13:43.901155Z","iopub.execute_input":"2025-07-09T02:13:43.901507Z","iopub.status.idle":"2025-07-09T02:13:44.196929Z","shell.execute_reply.started":"2025-07-09T02:13:43.901482Z","shell.execute_reply":"2025-07-09T02:13:44.195858Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass IMDBDataset(Dataset):\n    def __init__(self, df):\n        self.X = df.token.values\n        self.y = df.label.values\n        \n    def __getitem__(self, idx):\n        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n        \n    def __len__(self):\n        return len(self.y)\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df  = train_test_split(imdb_df, test_size=0.2, random_state=42)\n\ntrain_dataset = IMDBDataset(train_df)\ntest_dataset = IMDBDataset(test_df)\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n                n_layers, bidirectional, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n                          bidirectional=bidirectional, dropout=dropout,\n                          batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.bidirectional = bidirectional\n         \n\n    def forward(self, text):\n        x = self.embedding(text)\n        x = self.dropout(x)\n        _, (x, _) = self.rnn(x)\n        if self.bidirectional:\n            torch.cat((x[-2, :, :], x[-1, :, :]), dim=1)\n        else:\n            x = x[-1, :, :]\n        out = self.fc(x)\n        return out\n\nVOCAB_SIZE = 328232\nEMBEDDING_DIM = 20\nHIDDEN_DIM = 128\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = False \nDROPOUT = 0.5\nmodel = RNN(VOCAB_SIZE, EMBEDDING_DIM,HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,DROPOUT )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:13:46.786828Z","iopub.execute_input":"2025-07-09T02:13:46.787150Z","iopub.status.idle":"2025-07-09T02:13:46.903688Z","shell.execute_reply.started":"2025-07-09T02:13:46.787126Z","shell.execute_reply":"2025-07-09T02:13:46.902511Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n                n_layers, bidirectional, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n                          bidirectional=bidirectional, dropout=dropout,\n                          batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.bidirectional = bidirectional\n         \n\n    def forward(self, text):\n        x = self.embedding(text)\n        x = self.dropout(x)\n        _, (x, _) = self.rnn(x)\n        if self.bidirectional:\n            torch.cat((x[-2, :, :], x[-1, :, :]), dim=1)\n        else:\n            x = x[-1, :, :]\n        out = self.fc(x)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:12:57.873135Z","iopub.execute_input":"2025-07-09T02:12:57.875212Z","iopub.status.idle":"2025-07-09T02:12:57.885854Z","shell.execute_reply.started":"2025-07-09T02:12:57.875156Z","shell.execute_reply":"2025-07-09T02:12:57.883876Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"VOCAB_SIZE = 328232\nEMBEDDING_DIM = 20\nHIDDEN_DIM = 128\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = False \nDROPOUT = 0.5\nmodel = RNN(VOCAB_SIZE, EMBEDDING_DIM,HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,DROPOUT )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:13:03.359433Z","iopub.execute_input":"2025-07-09T02:13:03.359795Z","iopub.status.idle":"2025-07-09T02:13:03.421467Z","shell.execute_reply.started":"2025-07-09T02:13:03.359730Z","shell.execute_reply":"2025-07-09T02:13:03.420270Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"import torch\nif torch.cuda.is_available():\n    DEVICE = torch.device('cuda')\nelse:\n    DEVICE = torch.device('cpu')\nprint('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)\n\n\nmodel = model.to(DEVICE)\n\n\nimport torch.optim as optim\noptimizer =optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.BCEWithLogitsLoss().to(DEVICE)\n\n\n\n\ndef binary_accuracy(preds, y):\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float()\n    return correct.sum() / len(correct)\ndef train(model, loader, optimizer, criterion):\n    model.train()\n    epoch_loss = 0\n    epoch_acc = 0\n    for text, labels in loader:\n        text, labels = text.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        predictions = model(text).squeeze(1)\n        loss = criterion(predictions, labels)\n        acc = binary_accuracy(predictions, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n    return epoch_loss / len(loader), epoch_acc / len(loader)\n\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    with torch.no_grad():\n        for text, labels in loader:\n            text, labels = text.to(DEVICE), labels.to(DEVICE)\n            predictions = model(text).squeeze(1)\n            loss = criterion(predictions, labels)\n            acc = binary_accuracy(predictions, labels)\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n    return epoch_loss / len(loader), epoch_acc / len(loader)\n\n\nfor epoch in range(5):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n    \n    print(f'Epoch {epoch+1}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T02:13:51.518125Z","iopub.execute_input":"2025-07-09T02:13:51.518461Z","iopub.status.idle":"2025-07-09T02:13:51.572275Z","shell.execute_reply.started":"2025-07-09T02:13:51.518439Z","shell.execute_reply":"2025-07-09T02:13:51.570788Z"}},"outputs":[{"name":"stdout","text":"Using PyTorch version: 2.6.0+cu124  Device: cpu\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3712594600.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3712594600.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/983611048.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"],"ename":"IndexError","evalue":"index out of range in self","output_type":"error"}],"execution_count":111},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}